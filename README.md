# Live VLM WebUI

**A universal web interface for real-time Vision Language Model interaction and benchmarking.**

Stream your webcam to any VLM and get live AI-powered analysis - perfect for testing models, benchmarking performance, and exploring vision AI capabilities across multiple domains and hardware platforms.

![Live VLM WebUI Screenshot](./docs/images/chrome_app_running.png)

---

## ğŸš€ Quick Start (Easiest Way!)

**Works on PC (x86_64), DGX Spark (ARM64), Jetson Orin, and Jetson Thor** - same simple steps:

```bash
# 1. Clone the repository
git clone https://github.com/nvidia-ai-iot/live-vlm-webui.git
cd live-vlm-webui

# 2. Run the auto-detection script
./start_container.sh
```

That's it! The script will:
- âœ… Auto-detect your platform (PC x86_64, Jetson Orin, or Jetson Thor)
- âœ… Pull the appropriate pre-built image from GitHub Container Registry
- âœ… Configure GPU access automatically
- âœ… Start the container with correct settings

**Access the WebUI:** Open **`https://localhost:8090`** in your browser

> [!NOTE]
> You'll need a VLM backend running (Ollama, vLLM, etc.). See [VLM Backend Setup](#-setting-up-your-vlm-backend) below.

### Available Pre-built Images

| Platform | Image Tag | Pull Command |
|----------|-----------|--------------|
| **PC (x86_64) / DGX Spark** | `latest` | `docker pull ghcr.io/nvidia-ai-iot/live-vlm-webui:latest` |
| **Jetson Orin** | `latest-jetson-orin` | `docker pull ghcr.io/nvidia-ai-iot/live-vlm-webui:latest-jetson-orin` |
| **Jetson Thor** | `latest-jetson-thor` | `docker pull ghcr.io/nvidia-ai-iot/live-vlm-webui:latest-jetson-thor` |

> [!TIP]
> The `latest` tag is a **multi-arch image** that automatically selects the correct architecture:
> - `linux/amd64` for x86_64 PC and DGX systems
> - `linux/arm64` for DGX Spark (ARM64 SBSA server)

---

## ğŸ¥ WebUI Usage

Once the server is running, access the web interface at **`https://localhost:8090`**

### Accepting the SSL Certificate

| 1ï¸âƒ£ Click **"Advanced"** button | 2ï¸âƒ£ Click **"Proceed to localhost (unsafe)"** | 3ï¸âƒ£ Allow camera access when prompted |
|:---:|:---:|:---:|
| ![Chrome Advanced](./docs/images/chrome_advanced.png) | ![Chrome Proceed](./docs/images/chrome_proceed.png) | ![Chrome Webcam Access](./docs/images/chrome_webcam_access.png) |

### Interface Overview

**Left Sidebar Controls:**

<img src="./docs/images/usage_left_pane.png" align="left" width="180px" style="margin-right: 50px; margin-bottom: 10px;">

#### **ğŸŒ VLM API Configuration**
  - Set **API Base URL**, API Key, and **Model**
    - ğŸ”„ Refresh models button - Auto-detect available models
    - â• Download button (coming soon)

#### **ğŸ“¹ Camera Control**
  - Dropdown menu lists all detected cameras
  - Switch cameras on-the-fly without restarting
  - **START/STOP** buttons for analysis control
  - **Frame Interval**: Process every N frames (1-3600)
    - Lower (5-30) = more frequent, higher GPU usage
    - Higher (60-300) = less frequent, power saving

#### **âœï¸ Prompt Editor**
  - 10+ preset prompts (scene description, object detection, safety, OCR, etc.)
  - Write custom prompts
  - Adjust **Max Tokens** for response length (1-4096)

<br clear="left">

<img src="./docs/images/usage_main_pane.png" align="right" width="240px" style="margin-left: 50px; margin-bottom: 10px;">

**Main Content Area:**

- **VLM Output Card** - Real-time analysis results:
  - Model name and inference latency metrics
  - Current prompt display (gray box with green accent)
  - Generated text output
- **Video Feed** - Live webcam with ğŸ”„ mirror toggle button
- **System Stats Card** - Live monitoring:
  - System info: hostname (CPU model) with GPU name
  - GPU utilization and VRAM with progress bars
  - CPU and RAM stats
  - Sparkline graphs (60-second history)

<br clear="right">

**Header:**
- **ğŸ¥ Live VLM WebUI** - Logo and title
- **Connection Status** - WebSocket connectivity indicator
- **âš™ï¸ Settings** - Advanced configuration modal (WebRTC, latency thresholds, debugging)
- **ğŸŒ™/â˜€ï¸ Theme Toggle** - Switch between Light/Dark modes

![](./docs/images/usage_header.png)

---

## ğŸ’» Local Installation (Versatile, Works on Mac)

**For developers who want full control and customization:**

```bash
# 1. Clone the repository
git clone https://github.com/nvidia-ai-iot/live-vlm-webui.git
cd live-vlm-webui

# 2. Create virtual environment
python3 -m venv .venv
source .venv/bin/activate

# 3. Install dependencies
pip install -r requirements.txt

# 4. Generate SSL certificates
./generate_cert.sh

# 5. Start the server
./start_server.sh
```

**Access the WebUI:** Open **`https://localhost:8090`**

**Platforms supported:**
- âœ… Linux (x86_64, ARM64) - fully tested
- âœ… macOS (Intel, Apple Silicon) - fully tested
- âš ï¸ Windows - WSL2 recommended, native Windows possible but requires additional setup (FFmpeg, build tools)

---

## ğŸ¤– Setting Up Your VLM Backend

Choose the VLM backend that fits your needs:

### Quick Comparison

| Backend | Setup Difficulty | Speed | Quality | GPU Required |
|---------|-----------------|-------|---------|--------------|
| **Ollama** | â­ Easy | ğŸŸ¢ Fast | ğŸŸ¢ Good | Yes (local) |
| **vLLM** | â­â­ Medium | ğŸŸ¢ğŸŸ¢ Fastest | ğŸŸ¢ğŸŸ¢ Excellent | Yes (local) |
| **SGLang** | â­â­ Medium | ğŸŸ¢ Fast | ğŸŸ¢ğŸŸ¢ Excellent | Yes (local) |
| **NVIDIA API Catalog** | â­ Easy | ğŸŸ¡ Medium | ğŸŸ¢ğŸŸ¢ğŸŸ¢ Best | No |

### Option A: Ollama (Recommended for Beginners)

```bash
# Install from https://ollama.ai/download
# Pull a vision model
ollama pull llama3.2-vision:11b

# Start server
ollama serve
```

**Best for:** Quick start, easy model management

### Option B: vLLM (Recommended for Performance)

```bash
# Install vLLM
pip install vllm

# Start server
python -m vllm.entrypoints.openai.api_server \
  --model meta-llama/Llama-3.2-11B-Vision-Instruct \
  --port 8000
```

**Best for:** Production deployments, high throughput

### Option C: NVIDIA API Catalog (No GPU Required)

1. Visit [NVIDIA API Catalog](https://build.nvidia.com/)
2. Get API key from a vision model page
3. Configure in WebUI:
   - API Base: `https://ai.api.nvidia.com/v1/gr`
   - API Key: `nvapi-YOUR_KEY`
   - Model: `meta/llama-3.2-90b-vision-instruct`

**Best for:** Cloud-based inference, instant access

**ğŸ“— Detailed Guide:** [VLM Backend Setup](./docs/setup/vlm-backends.md)

---

## ğŸ³ Quick Deploy: Docker Compose with VLM Backend

**For PC and DGX Spark users who want VLM + WebUI in one command:**

> [!TIP]
> `start_docker_compose.sh` automatically detects your platform, checks Docker installation, and selects the correct profile. Just run it!

### With Ollama (Easiest, No API Keys Required)

**Using the launcher script (recommended):**
```bash
./start_docker_compose.sh ollama

# Pull a vision model after startup
docker exec ollama ollama pull llama3.2-vision:11b
```

**Or manually with docker compose:**
```bash
docker compose --profile ollama up

# Pull a vision model
docker exec ollama ollama pull llama3.2-vision:11b
```

> [!TIP]
> Backend-centric profiles make it easy: `--profile ollama`, `--profile vllm` (future), etc.

Includes:
- âœ… Ollama for easy model management
- âœ… Live VLM WebUI for real-time interaction
- âœ… No API keys required

### With NVIDIA NIM + Cosmos-Reason1-7B (Advanced)

> [!TIP]
> Cosmos-Reason1-7B is the default NIM model because it's the only NVIDIA VLM NIM that supports both x86_64 (PC) and ARM64 (DGX Spark, Jetson Thor) architectures. Other NIM models like Llama-3.2-90B-Vision and Nemotron are x86_64-only.

**Using the launcher script (recommended):**
```bash
# Get NGC API Key from https://org.ngc.nvidia.com/setup/api-key
export NGC_API_KEY=<your-key>

./start_docker_compose.sh nim
```

**Or manually with docker compose:**
```bash
export NGC_API_KEY=<your-key>
docker compose --profile nim up
```

Includes:
- âœ… NVIDIA NIM serving Cosmos-Reason1-7B with reasoning capabilities
- âœ… Production-grade inference
- âœ… Advanced VLM with planning and anomaly detection

> [!IMPORTANT]
> NIM requires NGC API Key and downloads ~10-15GB on first run. Requires NVIDIA driver 565+ (CUDA 12.9 support).

**ğŸ“— Detailed Guide:** [Docker Compose Setup Details](./docs/setup/docker-compose-details.md)

---

## ğŸ“š Documentation

### For Users
- ğŸ“– [VLM Backend Setup](./docs/setup/vlm-backends.md) - Detailed guide for Ollama, vLLM, SGLang, NVIDIA API
- ğŸ‹ [Docker Compose Details](./docs/setup/docker-compose-details.md) - Complete stack setup with Ollama or NIM
- ğŸ› ï¸ [Manual Docker Deployment](./docs/setup/docker-manual.md) - Advanced Docker configurations
- âš™ï¸ [Advanced Configuration](./docs/usage/advanced-configuration.md) - Performance tuning, custom prompts, API compatibility

### For Developers
- ğŸ”¨ [Building Docker Images](./docs/development/building-images.md) - Build platform-specific images for GHCR
- ğŸ§‘â€ğŸ’» [Contributing Guide](./CONTRIBUTING.md) - How to contribute to the project

### Help & Support
- ğŸ†˜ [Troubleshooting Guide](./docs/troubleshooting.md) - Common issues and solutions
- ğŸ’¬ [GitHub Issues](https://github.com/nvidia-ai-iot/live-vlm-webui/issues) - Bug reports and feature requests
- ğŸŒ [NVIDIA Developer Forums](https://forums.developer.nvidia.com/) - Community support

---

## âœ¨ Key Features

### Core Functionality
- ğŸ¥ **Real-time WebRTC streaming** - Low-latency bidirectional video
- ğŸ”Œ **OpenAI-compatible API** - Works with vLLM, SGLang, Ollama, TGI, or any vision API
- ğŸ“ **Interactive prompt editor** - 10+ preset prompts + custom prompts
- âš¡ **Async processing** - Smooth video while VLM processes frames in background
- ğŸ”§ **Flexible deployment** - Local inference or cloud APIs

### UI & Visualization
- ğŸ¨ **Modern NVIDIA-themed UI** - Professional design with NVIDIA green accents
- ğŸŒ“ **Light/Dark theme toggle** - Automatic preference persistence
- ğŸ“Š **Live system monitoring** - Real-time GPU, VRAM, CPU, RAM stats with sparkline charts
- â±ï¸ **Inference metrics** - Live latency tracking (last, average, total count)
- ğŸª **Video mirroring** - Toggle button overlay on camera view
- ğŸ“± **Compact layout** - Single-screen design

### Platform Support
- ğŸ’» **Cross-platform monitoring** - Auto-detects NVIDIA GPUs (NVML), Apple Silicon, AMD (coming soon)
- ğŸ–¥ï¸ **Dynamic system detection** - CPU model name and hostname
- ğŸ”’ **HTTPS support** - Self-signed certificates for secure webcam access
- ğŸŒ **Universal compatibility** - PC (x86_64), DGX Spark (ARM64 SBSA), Jetson (Orin, Thor), Mac
- ğŸ—ï¸ **Multi-arch Docker images** - Single image works across x86_64 and ARM64 architectures

---

## ğŸ—ºï¸ Use Cases

- ğŸ¬ **Content Creation** - Live scene analysis for video production
- ğŸ”’ **Security** - Real-time monitoring and alert generation
- â™¿ **Accessibility** - Visual assistance for visually impaired users
- ğŸ® **Gaming** - AI game master or interactive experiences
- ğŸ¥ **Healthcare** - Activity monitoring, fall detection
- ğŸ­ **Industrial** - Quality control, safety monitoring
- ğŸ“š **Education** - Interactive learning experiences
- ğŸ¤– **Robotics** - Visual feedback for robot control

---

## ğŸ› ï¸ Troubleshooting

### Quick Fixes

**Camera not accessible?**
- Use HTTPS (not HTTP): `./start_server.sh` or `--ssl-cert cert.pem --ssl-key key.pem`
- Accept the self-signed certificate warning (Advanced â†’ Proceed)

**Can't connect to VLM?**
- Check VLM is running: `curl http://localhost:8000/v1/models` (vLLM) or `curl http://localhost:11434/v1/models` (Ollama)
- Use `--network host` in Docker for local VLM services

**GPU stats show "N/A"?**
- PC: Add `--gpus all` when running Docker
- Jetson: Add `--privileged -v /run/jtop.sock:/run/jtop.sock:ro`

**Slow performance?**
- Use smaller model (llava:7b instead of llava:34b)
- Increase Frame Processing Interval (60+ frames)
- Reduce Max Tokens (50-100 instead of 512)

## ğŸ”§ Other Ways to Set Up

### Option 1: Docker Compose (Complete Stack)

For launching the WebUI alongside a VLM backend (Ollama or NVIDIA NIM) in a single stack:

**Using the launcher script (recommended):**
```bash
# Ollama (easy, no API keys)
./start_docker_compose.sh ollama

# NVIDIA NIM (advanced, requires NGC API key)
export NGC_API_KEY=<your-key>
./start_docker_compose.sh nim
```

**Manual docker compose:**
```bash
# Ollama
docker compose --profile ollama up

# NVIDIA NIM
export NGC_API_KEY=<your-key>
docker compose --profile nim up
```

**ğŸ“— Full Guide:** [Docker Compose Details](./docs/setup/docker-compose-details.md) - Includes NIM model selection, troubleshooting, and platform-specific instructions.

### Option 2: Manual Docker Run

For more control over Docker configurations, see [Manual Docker Setup](./docs/setup/docker-manual.md).

### Option 3: Local Installation (Most Flexible)

For development or custom setups, install directly without Docker:

**Requirements:**
- Python 3.10+
- NVIDIA GPU with CUDA support (for GPU monitoring)
- FFmpeg (for video processing)

**Quick setup:**
```bash
git clone https://github.com/nvidia-ai-iot/live-vlm-webui.git
cd live-vlm-webui
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
./generate_cert.sh
./start_server.sh
```

**ğŸ“— See:** Full instructions above in [Quick Start â†’ Option 2: Local Installation](#option-2-local-installation-versatile-works-on-mac)

---

## ğŸ¤ Contributing

Contributions welcome! Areas for improvement:
- ğŸ”„ Apple Silicon GPU monitoring
- ğŸ”„ AMD GPU monitoring
- ğŸ“¹ Recording functionality
- ğŸ¥ Multiple simultaneous camera support
- ğŸ”Š Audio description output (TTS)
- ğŸ“± Mobile app support
- ğŸ† Benchmark mode with side-by-side comparison
- ğŸ“Š Export analysis results (JSON, CSV)
- âš¡ **Hardware-accelerated video processing on Jetson** - Use NVENC/NVDEC

See [Contributing Guide](./CONTRIBUTING.md) for details.

---

## ğŸ“¦ Project Structure

```
live-vlm-webui/
â”œâ”€â”€ server.py            # Main WebRTC server with WebSocket support
â”œâ”€â”€ video_processor.py   # Video frame processing and VLM integration
â”œâ”€â”€ gpu_monitor.py       # Cross-platform GPU/system monitoring
â”œâ”€â”€ index.html           # Frontend web UI
â”œâ”€â”€ requirements.txt     # Python dependencies
â”œâ”€â”€ start_server.sh      # Quick start script with SSL
â”œâ”€â”€ start_container.sh   # Auto-detection Docker launcher
â”œâ”€â”€ generate_cert.sh     # SSL certificate generation
â”œâ”€â”€ Dockerfile           # Docker image for x86_64 PC
â”œâ”€â”€ Dockerfile.jetson-orin  # Docker image for Jetson Orin
â”œâ”€â”€ Dockerfile.jetson-thor  # Docker image for Jetson Thor
â”œâ”€â”€ docker-compose.yml      # Unified stack (Ollama + NIM + future backends)
â”œâ”€â”€ docs/                # Detailed documentation
â”‚   â”œâ”€â”€ setup/           # Setup guides
â”‚   â”œâ”€â”€ usage/           # Usage guides
â”‚   â”œâ”€â”€ development/     # Developer guides
â”‚   â””â”€â”€ troubleshooting.md
â””â”€â”€ README.md           # This file
```

---

## ğŸ“„ License

MIT License - Feel free to use and modify for your projects!

---

## ğŸ™ Acknowledgments

- Built with [aiortc](https://github.com/aiortc/aiortc) - Python WebRTC implementation
- Compatible with [vLLM](https://github.com/vllm-project/vllm), [SGLang](https://github.com/sgl-project/sglang), and [Ollama](https://ollama.ai/)
- Inspired by the growing ecosystem of open-source vision language models, including [NanoVLM](https://dusty-nv.github.io/NanoLLM/)

---

## ğŸ“ Citation

If you use this in your research or project, please cite:

```bibtex
@software{live_vlm_webui,
  title = {Live VLM WebUI: Real-time Vision AI Interaction},
  year = {2025},
  url = {https://github.com/nvidia-ai-iot/live-vlm-webui}
}
```
